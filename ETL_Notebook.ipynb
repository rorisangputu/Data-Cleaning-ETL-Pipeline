{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df5245-87d3-421e-96d6-e9e901e2601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c02074-2bce-4866-a076-428241fd2f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_data_cleaning(data_path, data_name):\n",
    "    print(f\"\\n🔍 Loading dataset: {data_path}\")\n",
    "\n",
    "    # Create log file with timestamp in name\n",
    "    log_filename = f\"{data_name}_cleaning_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "    logging.basicConfig(\n",
    "        filename=log_filename,\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    logging.info(f\"Starting ETL cleaning for dataset: {data_path}\")\n",
    "\n",
    "    # Data Loading\n",
    "    if not os.path.exists(data_path):\n",
    "        print(\"❌ Path not found! Please double-check and try again.\")\n",
    "        return None, None\n",
    "\n",
    "    if data_path.endswith('.csv'): #checks if file is csv\n",
    "        df = pd.read_csv(data_path, encoding_errors='ignore') #importing file using read csv\n",
    "    elif data_path.endswith('.xlsx'): #checks if file is excel\n",
    "        df = pd.read_excel(data_path) #importing file using read excel\n",
    "    else:\n",
    "        #Error handling \n",
    "        print(\"❌ Unsupported file type. Please use CSV or Excel.\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"✅ Dataset loaded. {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "    logging.info(f\"Dataset loaded successfully. {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "\n",
    "    # Handle duplicates\n",
    "    total_duplicates = df.duplicated().sum()\n",
    "    print(f\"🔁 Found {total_duplicates} duplicate rows.\")\n",
    "\n",
    "    duplicates = None  # Initialize duplicates\n",
    "    if total_duplicates > 0:\n",
    "        duplicates = df[df.duplicated()]\n",
    "        duplicates.to_csv(f\"{data_name}_duplicates.csv\", index=False)\n",
    "        print(f\"📝 Duplicates saved as: {data_name}_duplicates.csv\")\n",
    "        df = df.drop_duplicates()\n",
    "    \n",
    "    # After Handle duplicates\n",
    "    print(f\"Summary: {total_duplicates} duplicates found. {df.shape[0]} rows remain after deduplication.\")\n",
    "    \n",
    "    logging.info(f\"Duplicates handled: {total_duplicates} found, {df.shape[0]} rows remain after deduplication.\")\n",
    "    if total_duplicates > 0:\n",
    "        logging.info(f\"Duplicates saved to {data_name}_duplicates.csv\")\n",
    "\n",
    "    # Cleaning name column: removing titles, impute missing\n",
    "    if 'name' in df.columns:\n",
    "        df['name'] = df['name'].str.replace(r'^(Dr|Mr|Mrs|Ms|MD|DDS|DVM)\\.?\\s+', '', regex=True).str.strip()\n",
    "        df['name'] = df['name'].fillna(\"Unknown\")  # New: Impute missing names\n",
    "    \n",
    "    # Post cleaning name column\n",
    "    print(f\"Summary: Missing names imputed: {df['name'].eq('Unknown').sum()}. Titles removed from names.\")\n",
    "    logging.info(f\"Name cleaning: {df['name'].eq('Unknown').sum()} missing names imputed, titles removed.\")\n",
    "    \n",
    "    # Cleaning email column: changing to lowercase, strip, impute missing\n",
    "    if 'email' in df.columns:\n",
    "        df['email'] = df['email'].str.lower().str.strip()\n",
    "        df['email'] = df['email'].fillna(\"no_email@unknown.com\")  # New: Impute missing emails\n",
    "    \n",
    "    # Post cleaning email column\n",
    "    print(f\"Summary: Missing emails imputed: {df['email'].eq('no_email@unknown.com').sum()}. Emails standardized to lowercase.\")\n",
    "    logging.info(f\"Email cleaning: {df['email'].eq('no_email@unknown.com').sum()} missing emails imputed, standardized to lowercase.\")\n",
    "    \n",
    "    # Converting birthdate to datetime, drop invalid, adding age column\n",
    "    if 'birthdate' in df.columns:\n",
    "\n",
    "        df['birthdate'] = pd.to_datetime(df['birthdate'], errors='coerce')\n",
    "        df = df[df['birthdate'].notna()]\n",
    "        \n",
    "        # New column: Ages calculated\n",
    "        df['age'] = df['birthdate'].apply(lambda x: datetime.now().year - x.year)\n",
    "\n",
    "        # New column: Flags suspicious ages\n",
    "        df['age_flag'] = df['age'].apply(lambda x: 'Suspicious' if x < 0 or x > 100 else 'Valid')\n",
    "        \n",
    "        # New column: Adds birthdate string format\n",
    "        df['birthdate_str'] = df['birthdate'].dt.strftime('%d-%m-%Y')\n",
    "\n",
    "    # Post conversion messaging\n",
    "    print(f\"Summary: Invalid birthdates dropped: {df['birthdate'].isna().sum()}. Suspicious ages flagged: {df['age_flag'].eq('Suspicious').sum()}. Birthdate string format added.\")\n",
    "    logging.info(f\"Birthdate processing: {df['birthdate'].isna().sum()} invalid birthdates dropped, {df['age_flag'].eq('Suspicious').sum()} suspicious ages flagged, string format (dd-mm-yyyy) added.\")\n",
    "    \n",
    "    # Clean and validate signup_date, add days since signup\n",
    "    if 'signup_date' in df.columns:\n",
    "        df['signup_date'] = pd.to_datetime(df['signup_date'], errors='coerce')\n",
    "        df = df[df['signup_date'].notna()]  #Drop missing signup_date\n",
    "        today = pd.Timestamp.today()\n",
    "        df = df[df['signup_date'] <= today]  #Drop future signup dates\n",
    "        df['days_since_signup'] = (today - df['signup_date']).dt.days  # Days since signup\n",
    "        \n",
    "        # Making signup_date string format in a different column\n",
    "        df['signup_date_str'] = df['signup_date'].dt.strftime('%d-%m-%Y')\n",
    "    \n",
    "    # Post cleaning and validation messaging\n",
    "    print(f\"Summary: Invalid signup dates dropped: {df['signup_date'].isna().sum()}. Future signup dates removed: {df['signup_date'].gt(today).sum()}. Days since signup and string format added.\")\n",
    "    logging.info(f\"Signup date processing: {df['signup_date'].isna().sum()} invalid signup dates dropped, {df['signup_date'].gt(today).sum()} future signup dates removed, days_since_signup and string format (dd-mm-yyyy) added.\")\n",
    "\n",
    "    # Cleaning purchase_amount: convert to numeric, fill NAs with mean (instead of 0)\n",
    "    if 'purchase_amount' in df.columns:\n",
    "        df['purchase_amount'] = pd.to_numeric(df['purchase_amount'], errors='coerce')\n",
    "        df['purchase_amount'] = df['purchase_amount'].fillna(df['purchase_amount'].mean())  # New: Use mean\n",
    "    \n",
    "    # Post cleaning messaging\n",
    "    print(f\"Summary: Non-numeric purchase_amount values converted to NaN: {df['purchase_amount'].isna().sum()}. Missing values filled with mean: {df['purchase_amount'].isna().sum()}.\")\n",
    "    logging.info(f\"Purchase amount cleaning: {df['purchase_amount'].isna().sum()} non-numeric values converted to NaN, missing values filled with mean.\")\n",
    "\n",
    "    # Normalizing category, drop missing\n",
    "    if 'category' in df.columns:\n",
    "        df['category'] = df['category'].str.lower().str.strip()\n",
    "        df = df[df['category'].notna()]  # Drop missing category\n",
    "        category_map = {\n",
    "            'electronics': 'Electronics', 'electronic': 'Electronics',\n",
    "            'clothing': 'Clothing', 'clothes': 'Clothing',\n",
    "            'furniture': 'Furniture', 'furnitures': 'Furniture',\n",
    "            'toys': 'Toys', 'toy': 'Toys',\n",
    "            'books': 'Books', 'book': 'Books',\n",
    "            'groceries': 'Groceries', 'grocery': 'Groceries',\n",
    "            'sports': 'Sports', 'sport': 'Sports',\n",
    "            'beauty': 'Beauty'\n",
    "        }\n",
    "        df['category'] = df['category'].map(category_map).fillna(df['category'])\n",
    "    \n",
    "    # Post normalisation messaging\n",
    "    print(f\"Summary: Missing categories dropped: {df['category'].isna().sum()}. Categories standardized: {df['category'].value_counts().to_dict()}.\")\n",
    "    logging.info(f\"Category normalization: {df['category'].isna().sum()} missing categories dropped, standardized categories: {df['category'].value_counts().to_dict()}.\")\n",
    "\n",
    "    # Trim whitespace from all string columns\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].str.strip()\n",
    "    # After Trim whitespace from all string columns\n",
    "    print(f\"Summary: Whitespace trimmed from string columns: {list(df.select_dtypes(include='object').columns)}.\")\n",
    "\n",
    "\n",
    "    # Checking data consistency: flagging rows where signup_date is before birthdate\n",
    "    if 'birthdate' in df.columns and 'signup_date' in df.columns:\n",
    "        df['date_consistency'] = df.apply(\n",
    "            lambda x: 'Invalid' if pd.notna(x['birthdate']) and pd.notna(x['signup_date']) \n",
    "            and x['signup_date'] < x['birthdate'] else 'Valid', axis=1)\n",
    "    \n",
    "    # Post check messaging\n",
    "    print(f\"Summary: Inconsistent dates flagged: {df['date_consistency'].eq('Invalid').sum()} invalid signup_date vs. birthdate cases.\")\n",
    "    logging.info(f\"Data consistency check: {df['date_consistency'].eq('Invalid').sum()} invalid signup_date vs. birthdate cases flagged.\")\n",
    "    \n",
    "    # Save cleaned data\n",
    "    clean_file = f\"{data_name}_cleaned.csv\"\n",
    "    df.to_csv(clean_file, index=False)\n",
    "    print(f\"🎉 Dataset cleaned! Saved as: {clean_file}\")\n",
    "    print(f\"Final shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "    logging.info(f\"Dataset cleaned and saved as {clean_file}. Final shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "    return duplicates, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9841b3-572f-48b7-9347-5831c34a1250",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Welcome to ETL Data Cleaning!\")\n",
    "    data_path = input(\"📂 Enter dataset path: \")\n",
    "    data_name = input(\"📛 Enter dataset name: \")\n",
    "    duplicates, clean_data = etl_data_cleaning(data_path, data_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
